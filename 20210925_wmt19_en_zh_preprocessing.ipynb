{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.5.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.12.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "tf.config.set_soft_device_placement(True)\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = '20210925_wmt19_en_zh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Source: [data.statmt.org](http://data.statmt.org/news-commentary/v14/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-lab/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "Skipping line 5803: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 5804: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 12524: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 12525: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 12526: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 12528: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 32578: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 32603: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 32612: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 38687: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 40278: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 40751: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 40752: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 53670: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 53679: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 53680: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 59424: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 66350: field larger than field limit (131072). Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 70441: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 79651: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 90967: field larger than field limit (131072). Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 92390: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 92391: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 94542: field larger than field limit (131072). Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 96378: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 96379: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 101926: field larger than field limit (131072). Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 109275: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 122086: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 124982: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 126316: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 140484: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 154765: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 154792: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 165459: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 170118: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 175998: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 182568: field larger than field limit (131072). Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 190288: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 190289: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 191257: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 203590: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 207570: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 211623: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 211645: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 214161: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 214165: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 214184: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 214187: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 215915: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 238872: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 238901: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 238902: field larger than field limit (131072). Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 246615: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 251317: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 257207: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 257210: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 258562: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 269142: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 269143: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 269148: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 269149: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 269153: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 291134: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 299996: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 300001: '\t' expected after '\"'. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).\n",
      "Skipping line 9863: Expected 2 fields in line 9863, saw 3\n",
      "Skipping line 9865: Expected 2 fields in line 9865, saw 3\n",
      "Skipping line 9871: Expected 2 fields in line 9871, saw 3\n",
      "Skipping line 9881: Expected 2 fields in line 9881, saw 3\n",
      "Skipping line 9883: Expected 2 fields in line 9883, saw 3\n",
      "Skipping line 9885: Expected 2 fields in line 9885, saw 3\n",
      "Skipping line 9887: Expected 2 fields in line 9887, saw 3\n",
      "Skipping line 9890: Expected 2 fields in line 9890, saw 3\n",
      "Skipping line 20937: Expected 2 fields in line 20937, saw 3\n",
      "Skipping line 20941: Expected 2 fields in line 20941, saw 3\n",
      "Skipping line 20958: Expected 2 fields in line 20958, saw 3\n",
      "Skipping line 32613: Expected 2 fields in line 32613, saw 3\n",
      "Skipping line 43018: Expected 2 fields in line 43018, saw 3\n",
      "Skipping line 43048: Expected 2 fields in line 43048, saw 3\n",
      "Skipping line 43536: Expected 2 fields in line 43536, saw 3\n",
      "Skipping line 43542: Expected 2 fields in line 43542, saw 3\n",
      "Skipping line 43543: Expected 2 fields in line 43543, saw 3\n",
      "Skipping line 43559: Expected 2 fields in line 43559, saw 3\n",
      "Skipping line 43564: Expected 2 fields in line 43564, saw 3\n",
      "Skipping line 43567: Expected 2 fields in line 43567, saw 3\n",
      "Skipping line 46844: Expected 2 fields in line 46844, saw 3\n",
      "Skipping line 55595: Expected 2 fields in line 55595, saw 3\n",
      "Skipping line 55598: Expected 2 fields in line 55598, saw 3\n",
      "Skipping line 55599: Expected 2 fields in line 55599, saw 3\n",
      "Skipping line 55604: Expected 2 fields in line 55604, saw 3\n",
      "Skipping line 55610: Expected 2 fields in line 55610, saw 3\n",
      "Skipping line 55614: Expected 2 fields in line 55614, saw 3\n",
      "Skipping line 55616: Expected 2 fields in line 55616, saw 3\n",
      "Skipping line 55619: Expected 2 fields in line 55619, saw 3\n",
      "Skipping line 55631: Expected 2 fields in line 55631, saw 3\n",
      "Skipping line 57900: Expected 2 fields in line 57900, saw 3\n",
      "Skipping line 57903: Expected 2 fields in line 57903, saw 3\n",
      "Skipping line 57906: Expected 2 fields in line 57906, saw 3\n",
      "Skipping line 57916: Expected 2 fields in line 57916, saw 3\n",
      "Skipping line 57918: Expected 2 fields in line 57918, saw 3\n",
      "Skipping line 59401: Expected 2 fields in line 59401, saw 4\n",
      "Skipping line 59402: Expected 2 fields in line 59402, saw 4\n",
      "Skipping line 59407: Expected 2 fields in line 59407, saw 4\n",
      "Skipping line 59411: Expected 2 fields in line 59411, saw 4\n",
      "Skipping line 59415: Expected 2 fields in line 59415, saw 4\n",
      "Skipping line 59417: Expected 2 fields in line 59417, saw 4\n",
      "Skipping line 88505: Expected 2 fields in line 88505, saw 3\n",
      "Skipping line 89006: Expected 2 fields in line 89006, saw 3\n",
      "Skipping line 89008: Expected 2 fields in line 89008, saw 3\n",
      "Skipping line 89012: Expected 2 fields in line 89012, saw 3\n",
      "Skipping line 89013: Expected 2 fields in line 89013, saw 3\n",
      "Skipping line 91263: Expected 2 fields in line 91263, saw 3\n",
      "Skipping line 91268: Expected 2 fields in line 91268, saw 3\n",
      "Skipping line 91283: Expected 2 fields in line 91283, saw 3\n",
      "Skipping line 93067: Expected 2 fields in line 93067, saw 3\n",
      "Skipping line 93074: Expected 2 fields in line 93074, saw 3\n",
      "Skipping line 93076: Expected 2 fields in line 93076, saw 3\n",
      "Skipping line 93080: Expected 2 fields in line 93080, saw 3\n",
      "Skipping line 93086: Expected 2 fields in line 93086, saw 3\n",
      "Skipping line 93957: Expected 2 fields in line 93957, saw 4\n",
      "Skipping line 93962: Expected 2 fields in line 93962, saw 4\n",
      "Skipping line 93965: Expected 2 fields in line 93965, saw 4\n",
      "Skipping line 93967: Expected 2 fields in line 93967, saw 4\n",
      "Skipping line 93985: Expected 2 fields in line 93985, saw 4\n",
      "Skipping line 93990: Expected 2 fields in line 93990, saw 4\n",
      "Skipping line 105519: Expected 2 fields in line 105519, saw 3\n",
      "Skipping line 105521: Expected 2 fields in line 105521, saw 3\n",
      "Skipping line 105540: Expected 2 fields in line 105540, saw 3\n",
      "Skipping line 105980: Expected 2 fields in line 105980, saw 3\n",
      "Skipping line 105983: Expected 2 fields in line 105983, saw 3\n",
      "Skipping line 106007: Expected 2 fields in line 106007, saw 3\n",
      "Skipping line 106009: Expected 2 fields in line 106009, saw 3\n",
      "Skipping line 107423: Expected 2 fields in line 107423, saw 3\n",
      "Skipping line 116143: Expected 2 fields in line 116143, saw 3\n",
      "Skipping line 116150: Expected 2 fields in line 116150, saw 3\n",
      "Skipping line 116160: Expected 2 fields in line 116160, saw 3\n",
      "Skipping line 118847: Expected 2 fields in line 118847, saw 3\n",
      "Skipping line 118851: Expected 2 fields in line 118851, saw 3\n",
      "Skipping line 118863: Expected 2 fields in line 118863, saw 3\n",
      "Skipping line 119221: Expected 2 fields in line 119221, saw 4\n",
      "Skipping line 119222: Expected 2 fields in line 119222, saw 3\n",
      "Skipping line 125591: Expected 2 fields in line 125591, saw 3\n",
      "Skipping line 140921: Expected 2 fields in line 140921, saw 3\n",
      "Skipping line 140939: Expected 2 fields in line 140939, saw 3\n",
      "Skipping line 140941: Expected 2 fields in line 140941, saw 3\n",
      "Skipping line 141150: Expected 2 fields in line 141150, saw 3\n",
      "Skipping line 141157: Expected 2 fields in line 141157, saw 3\n",
      "Skipping line 141165: Expected 2 fields in line 141165, saw 3\n",
      "Skipping line 141173: Expected 2 fields in line 141173, saw 3\n",
      "Skipping line 141180: Expected 2 fields in line 141180, saw 3\n",
      "Skipping line 141185: Expected 2 fields in line 141185, saw 3\n",
      "Skipping line 145710: Expected 2 fields in line 145710, saw 3\n",
      "Skipping line 145713: Expected 2 fields in line 145713, saw 3\n",
      "Skipping line 145721: Expected 2 fields in line 145721, saw 3\n",
      "Skipping line 145725: Expected 2 fields in line 145725, saw 3\n",
      "Skipping line 150367: Expected 2 fields in line 150367, saw 3\n",
      "Skipping line 150372: Expected 2 fields in line 150372, saw 3\n",
      "Skipping line 150378: Expected 2 fields in line 150378, saw 3\n",
      "Skipping line 160790: Expected 2 fields in line 160790, saw 3\n",
      "Skipping line 160792: Expected 2 fields in line 160792, saw 3\n",
      "Skipping line 160795: Expected 2 fields in line 160795, saw 3\n",
      "Skipping line 160797: Expected 2 fields in line 160797, saw 3\n",
      "Skipping line 160800: Expected 2 fields in line 160800, saw 3\n",
      "Skipping line 160802: Expected 2 fields in line 160802, saw 3\n",
      "Skipping line 160804: Expected 2 fields in line 160804, saw 3\n",
      "Skipping line 160812: Expected 2 fields in line 160812, saw 3\n",
      "Skipping line 160816: Expected 2 fields in line 160816, saw 3\n",
      "Skipping line 160820: Expected 2 fields in line 160820, saw 3\n",
      "Skipping line 160821: Expected 2 fields in line 160821, saw 3\n",
      "Skipping line 165020: Expected 2 fields in line 165020, saw 3\n",
      "Skipping line 165021: Expected 2 fields in line 165021, saw 3\n",
      "Skipping line 168165: Expected 2 fields in line 168165, saw 3\n",
      "Skipping line 168178: Expected 2 fields in line 168178, saw 3\n",
      "Skipping line 168181: Expected 2 fields in line 168181, saw 3\n",
      "Skipping line 175943: Expected 2 fields in line 175943, saw 3\n",
      "Skipping line 175947: Expected 2 fields in line 175947, saw 3\n",
      "Skipping line 175970: Expected 2 fields in line 175970, saw 3\n",
      "Skipping line 180042: Expected 2 fields in line 180042, saw 3\n",
      "Skipping line 180243: Expected 2 fields in line 180243, saw 3\n",
      "Skipping line 180261: Expected 2 fields in line 180261, saw 3\n",
      "Skipping line 180264: Expected 2 fields in line 180264, saw 3\n",
      "Skipping line 180534: Expected 2 fields in line 180534, saw 3\n",
      "Skipping line 180553: Expected 2 fields in line 180553, saw 3\n",
      "Skipping line 180559: Expected 2 fields in line 180559, saw 3\n",
      "Skipping line 196505: Expected 2 fields in line 196505, saw 3\n",
      "Skipping line 196512: Expected 2 fields in line 196512, saw 3\n",
      "Skipping line 196518: Expected 2 fields in line 196518, saw 3\n",
      "Skipping line 196520: Expected 2 fields in line 196520, saw 3\n",
      "Skipping line 196528: Expected 2 fields in line 196528, saw 3\n",
      "Skipping line 196530: Expected 2 fields in line 196530, saw 3\n",
      "Skipping line 196541: Expected 2 fields in line 196541, saw 3\n",
      "Skipping line 196940: Expected 2 fields in line 196940, saw 3\n",
      "Skipping line 196947: Expected 2 fields in line 196947, saw 3\n",
      "Skipping line 196963: Expected 2 fields in line 196963, saw 3\n",
      "Skipping line 196966: Expected 2 fields in line 196966, saw 3\n",
      "Skipping line 196969: Expected 2 fields in line 196969, saw 3\n",
      "Skipping line 201430: Expected 2 fields in line 201430, saw 3\n",
      "Skipping line 201436: Expected 2 fields in line 201436, saw 3\n",
      "Skipping line 201444: Expected 2 fields in line 201444, saw 3\n",
      "Skipping line 201453: Expected 2 fields in line 201453, saw 3\n",
      "Skipping line 203288: Expected 2 fields in line 203288, saw 3\n",
      "Skipping line 203314: Expected 2 fields in line 203314, saw 3\n",
      "Skipping line 203321: Expected 2 fields in line 203321, saw 3\n",
      "Skipping line 207921: Expected 2 fields in line 207921, saw 3\n",
      "Skipping line 207929: Expected 2 fields in line 207929, saw 3\n",
      "Skipping line 207940: Expected 2 fields in line 207940, saw 3\n",
      "Skipping line 207942: Expected 2 fields in line 207942, saw 3\n",
      "Skipping line 216455: Expected 2 fields in line 216455, saw 3\n",
      "Skipping line 216463: Expected 2 fields in line 216463, saw 3\n",
      "Skipping line 216465: Expected 2 fields in line 216465, saw 3\n",
      "Skipping line 216467: Expected 2 fields in line 216467, saw 3\n",
      "Skipping line 216471: Expected 2 fields in line 216471, saw 3\n",
      "Skipping line 216472: Expected 2 fields in line 216472, saw 3\n",
      "Skipping line 231807: Expected 2 fields in line 231807, saw 3\n",
      "Skipping line 231808: Expected 2 fields in line 231808, saw 3\n",
      "Skipping line 231812: Expected 2 fields in line 231812, saw 3\n",
      "Skipping line 233648: Expected 2 fields in line 233648, saw 3\n",
      "Skipping line 233664: Expected 2 fields in line 233664, saw 3\n",
      "Skipping line 291013: Expected 2 fields in line 291013, saw 3\n",
      "Skipping line 291014: Expected 2 fields in line 291014, saw 3\n",
      "Skipping line 291032: Expected 2 fields in line 291032, saw 3\n",
      "Skipping line 291040: Expected 2 fields in line 291040, saw 3\n",
      "Skipping line 296514: Expected 2 fields in line 296514, saw 3\n",
      "Skipping line 296516: Expected 2 fields in line 296516, saw 3\n",
      "Skipping line 296519: Expected 2 fields in line 296519, saw 3\n",
      "Skipping line 296535: Expected 2 fields in line 296535, saw 3\n",
      "Skipping line 313295: Expected 2 fields in line 313295, saw 3\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv(\n",
    "    'news-commentary-v14.en-zh.tsv', \n",
    "    sep='\\t', \n",
    "    error_bad_lines=False,\n",
    "    skipfooter= 1,\n",
    "    header = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = corpus[0].to_numpy(dtype = str)\n",
    "zh = corpus[1].to_numpy(dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1929 or 1989?'\n",
      " 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.'\n",
      " 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.'\n",
      " 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.'\n",
      " 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).']\n"
     ]
    }
   ],
   "source": [
    "print(en[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1929年还是1989年?' '巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。'\n",
      " '一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。'\n",
      " '如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。'\n",
      " '目前的趋势是，要么是过度的克制（欧洲 ） ， 要么是努力的扩展（美国 ） 。']\n"
     ]
    }
   ],
   "source": [
    "print(zh[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the text into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tkr = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    ")\n",
    "en_tkr.fit_on_texts(['<bos>', '<eos>'])\n",
    "en_tkr.fit_on_texts(en)\n",
    "en_seq = en_tkr.texts_to_sequences(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46227\n",
      "46228\n"
     ]
    }
   ],
   "source": [
    "en_word2idx = eval(en_tkr.get_config()['word_index'])\n",
    "bosIdx = en_word2idx['<bos>']\n",
    "eosIdx = en_word2idx['<eos>']\n",
    "print(bosIdx)\n",
    "print(eosIdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the average sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.864398210418873\n",
      "11.40384685075576\n"
     ]
    }
   ],
   "source": [
    "en_seq_len = [len(s) for s in en_seq]\n",
    "print(np.mean(en_seq_len))\n",
    "print(np.std(en_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_seq_length = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.614 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "zh = [' '.join(list(jieba.cut(zh_seq, cut_all=False))) for zh_seq in zh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1929 年 还是 1989 年 ?', '巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。', '一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。', '如今 人们 的 心情 却是 沉重 多 了 ， 许多 人 开始 把 这次 危机 与 1929 年 和 1931 年 相比 ， 即使 一些 国家 政府 的 表现 仍然 似乎 把视 目前 的 情况 为 是 典型 的 而 看见 的 衰退 。', '目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲   ）   ，   要么 是 努力 的 扩展 （ 美国   ）   。']\n"
     ]
    }
   ],
   "source": [
    "print(zh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_tkr = tf.keras.preprocessing.text.Tokenizer()\n",
    "zh_tkr.fit_on_texts(zh)\n",
    "zh_seq = zh_tkr.texts_to_sequences(zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.058609735855537\n",
      "12.381971197590456\n"
     ]
    }
   ],
   "source": [
    "zh_seq_len = [len(s) for s in zh_seq]\n",
    "print(np.mean(zh_seq_len))\n",
    "print(np.std(zh_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_seq_length = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241964\n"
     ]
    }
   ],
   "source": [
    "keep = [[zh, en] for zh, en in zip(zh_seq, en_seq) if len(zh)<=zh_seq_length and len(en)<=en_seq_length-2]\n",
    "print(len(keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add BOS and EOS into decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_reduce_seq = [pair[0] for pair in keep]\n",
    "en_reduce_seq = [pair[1] for pair in keep]\n",
    "for i, seq in enumerate(en_reduce_seq):\n",
    "    en_reduce_seq[i] = [bosIdx]\n",
    "    en_reduce_seq[i].extend(seq)\n",
    "    en_reduce_seq[i].append(eosIdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dictionary and re-tokenize sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929 年 还是 1989 年\n"
     ]
    }
   ],
   "source": [
    "# English\n",
    "en_reduce = en_tkr.sequences_to_texts(en_reduce_seq)\n",
    "en_tkr = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    ")\n",
    "en_tkr.fit_on_texts(en_reduce)\n",
    "en_reduce_seq = en_tkr.texts_to_sequences(en_reduce)\n",
    "for i, seq in enumerate(en_reduce_seq):\n",
    "    en_reduce_seq[i].append(0)\n",
    "en_idx2word = eval(en_tkr.get_config()['index_word'])\n",
    "en_idx2word['0'] = '<pad>'\n",
    "en_word2idx = eval(en_tkr.get_config()['word_index'])\n",
    "en_word2idx['<pad>'] = '0'\n",
    "\n",
    "# Czech\n",
    "zh_reduce = zh_tkr.sequences_to_texts(zh_reduce_seq)\n",
    "zh_tkr = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    ")\n",
    "zh_tkr.fit_on_texts(zh_reduce)\n",
    "zh_reduce_seq = zh_tkr.texts_to_sequences(zh_reduce)\n",
    "zh_idx2word = eval(zh_tkr.get_config()['index_word'])\n",
    "zh_idx2word['0'] = '<pad>'\n",
    "zh_word2idx = eval(zh_tkr.get_config()['word_index'])\n",
    "zh_word2idx['<pad>'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', '1929', 'or', '1989', '<eos>', '<pad>']\n",
      "['1929', '年', '还是', '1989', '年']\n"
     ]
    }
   ],
   "source": [
    "print([ en_idx2word[str(i)] for i in en_reduce_seq[0]])\n",
    "print([ zh_idx2word[str(i)] for i in zh_reduce_seq[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num. of en. word: 57298\n",
      "num. of cs. word: 75420\n"
     ]
    }
   ],
   "source": [
    "en_num_words = len(en_word2idx)\n",
    "zh_num_words = len(zh_word2idx)\n",
    "print(f'num. of en. word: {en_num_words}')\n",
    "print(f'num. of cs. word: {zh_num_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_pad_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    en_reduce_seq,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241964, 33)\n"
     ]
    }
   ],
   "source": [
    "print(en_pad_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_pad_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    zh_reduce_seq,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241964, 35)\n"
     ]
    }
   ],
   "source": [
    "print(zh_pad_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pair = len(en_pad_seq)\n",
    "encoder_train = zh_pad_seq[int(num_pair*0.1):]\n",
    "decoder_train = en_pad_seq[int(num_pair*0.1):, :-1]\n",
    "teacher_train = en_pad_seq[int(num_pair*0.1):, 1:]\n",
    "encoder_vali  = zh_pad_seq[:int(num_pair*0.1)]\n",
    "decoder_vali  = en_pad_seq[:int(num_pair*0.1), :-1]\n",
    "teacher_vali  = en_pad_seq[:int(num_pair*0.1), 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217768, 35)\n",
      "(217768, 32)\n",
      "(217768, 32)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_train.shape)\n",
    "print(decoder_train.shape)\n",
    "print(teacher_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32 dim. English embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37970183, 79848120)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [list(e) for e in en_pad_seq]\n",
    "model = Word2Vec(\n",
    "    sentences=tmp, \n",
    "    vector_size=32, \n",
    "    window=5, \n",
    "    min_count=1, \n",
    "    workers=16,\n",
    "    sg = 1,\n",
    "    negative = 10,\n",
    ")\n",
    "model.build_vocab(tmp)\n",
    "model.train(tmp, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0179631 , -0.8287212 , -0.51448613,  0.17656817,  0.35869485,\n",
       "       -0.3403227 ,  0.19687517,  0.3582963 ,  0.05079257,  1.1925439 ,\n",
       "       -0.24706498, -0.7956718 ,  0.29024014, -0.0999219 , -0.85695   ,\n",
       "       -0.29522488, -0.8852313 ,  0.29105672, -1.5672745 , -0.08303724,\n",
       "       -0.21882707, -1.0520015 ,  0.73115075,  0.45493084,  0.48359972,\n",
       "       -0.4749396 , -0.9445318 ,  0.82705444, -0.7292918 , -0.18514132,\n",
       "       -0.043488  ,  0.1810614 ], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[5257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_emb32 = np.array([ model.wv[i] for i in range(en_num_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32 dim. Czech embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39123827, 84687400)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [list(e) for e in zh_pad_seq]\n",
    "model = Word2Vec(\n",
    "    sentences=tmp, \n",
    "    vector_size=32, \n",
    "    window=5, \n",
    "    min_count=1, \n",
    "    workers=16,\n",
    "    sg = 1,\n",
    "    negative = 10,\n",
    ")\n",
    "model.build_vocab(tmp)\n",
    "model.train(tmp, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_emb32 = np.array([ model.wv[i] for i in range(zh_num_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(encoder_train, open(f'{folder_name}/encoder_train.pkl','wb'))\n",
    "pickle.dump(decoder_train, open(f'{folder_name}/decoder_train.pkl','wb'))\n",
    "pickle.dump(teacher_train, open(f'{folder_name}/teacher_train.pkl','wb'))\n",
    "pickle.dump(encoder_vali,  open(f'{folder_name}/encoder_vali.pkl','wb'))\n",
    "pickle.dump(decoder_vali,  open(f'{folder_name}/decoder_vali.pkl','wb'))\n",
    "pickle.dump(teacher_vali,  open(f'{folder_name}/teacher_vali.pkl','wb'))\n",
    "\n",
    "pickle.dump(en_idx2word, open(f'{folder_name}/en_idx2word.pkl','wb'))\n",
    "pickle.dump(en_word2idx, open(f'{folder_name}/en_word2idx.pkl','wb'))\n",
    "pickle.dump(zh_idx2word, open(f'{folder_name}/zh_idx2word.pkl','wb'))\n",
    "pickle.dump(zh_word2idx, open(f'{folder_name}/zh_word2idx.pkl','wb'))\n",
    "\n",
    "pickle.dump(en_emb32, open(f'{folder_name}/en_emb32.pkl','wb'))\n",
    "pickle.dump(zh_emb32, open(f'{folder_name}/zh_emb32.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_train = pickle.load(open(f'{folder_name}/encoder_train.pkl', 'rb'))\n",
    "decoder_train = pickle.load(open(f'{folder_name}/decoder_train.pkl', 'rb'))\n",
    "teacher_train = pickle.load(open(f'{folder_name}/teacher_train.pkl', 'rb'))\n",
    "encoder_vali  = pickle.load(open(f'{folder_name}/encoder_vali.pkl', 'rb'))\n",
    "decoder_vali  = pickle.load(open(f'{folder_name}/decoder_vali.pkl', 'rb'))\n",
    "teacher_vali  = pickle.load(open(f'{folder_name}/teacher_vali.pkl', 'rb'))\n",
    "\n",
    "en_idx2word   = pickle.load(open(f'{folder_name}/en_idx2word.pkl','rb'))\n",
    "en_word2idx   = pickle.load(open(f'{folder_name}/en_word2idx.pkl','rb'))\n",
    "zh_idx2word   = pickle.load(open(f'{folder_name}/zh_idx2word.pkl','rb'))\n",
    "zh_word2idx   = pickle.load(open(f'{folder_name}/zh_word2idx.pkl','rb'))\n",
    "\n",
    "en_emb32    = pickle.load(open(f'{folder_name}/en_emb32.pkl', 'rb'))\n",
    "zh_emb32    = pickle.load(open(f'{folder_name}/zh_emb32.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2word(seq_tensor, idx2word):\n",
    "    return np.array([[idx2word[str(i)] for i in seq] for seq in seq_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['权衡', '和', '折衷', '永远', '是', '资本主义', '的', '真谛', '：', '我们', '必须',\n",
       "        '容忍', '为', '我们', '提供', '有效', '工具', '的', '赚钱', '企业', '的', '不',\n",
       "        '道德行为', '。', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "        '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'],\n",
       "       ['伊朗人', '可以', '用', '这些', '工具', '来', '反抗', '专政', '，', '西藏', '持',\n",
       "        '不同', '政见', '者', '也', '可以', '用', '这些', '工具', '来', '拯救', '他们',\n",
       "        '的', '文化', '。', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "        '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'],\n",
       "       ['这些', '工具', '也', '可以', '用来', '统计', '灭绝', '犹太人', '的', '数量', '，',\n",
       "        '逮捕', '中国', '的', '持', '不同', '政见', '者', '，', '或者', '破坏', '俄罗斯',\n",
       "        '的', '人权', '组织', '。', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "        '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'],\n",
       "       ['俄罗斯', '的', '微软', '或者', '中国', '的', '谷歌', '告诉', '我们', '资本主义', '并',\n",
       "        '不道德', '：', '而是', '更加', '注重', '效率', '。', '<pad>', '<pad>',\n",
       "        '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "        '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "        '<pad>'],\n",
       "       ['贪婪', '是', '企业家', '的', '定义', '：', '如果', '不', '贪婪', '，', '他们',\n",
       "        '就', '会', '陷入', '破产', '的', '结局', '。', '<pad>', '<pad>', '<pad>',\n",
       "        '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "        '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2word(encoder_train[5:10], zh_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
